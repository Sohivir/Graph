{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dbfed93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soham\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa83e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f8591ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686ef8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_matrix_aug = np.loadtxt(\"Aug_inc_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dadf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_matrix_aug = inc_matrix_aug.reshape(-1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65bca184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_matrix_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "108990f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes, num_edges = inc_matrix_aug.shape\n",
    "\n",
    "# --- Step 2: Convert to edge_index for PyG (multi-edges allowed) ---\n",
    "edge_list = []\n",
    "for j in range(num_edges):\n",
    "    col = inc_matrix_aug[:, j]\n",
    "    src = np.where(col == 1)[0]\n",
    "    dst = np.where(col == -1)[0]\n",
    "    if len(src) == 1 and len(dst) == 1:\n",
    "        edge_list.append((src[0], dst[0]))  # directed edge\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "x = torch.eye(45, dtype=torch.float)\n",
    "\n",
    "# --- Step 3: Create PyG Data object ---\n",
    "data_inp= Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# --- Step 4: Visualize with Pyvis ---\n",
    "# G = nx.MultiDiGraph()\n",
    "# edge_tuples = edge_index.t().tolist()\n",
    "# G.add_edges_from(edge_tuples)\n",
    "\n",
    "# # Assign label, color, and tooltip (identity vector)\n",
    "# for node in G.nodes():\n",
    "#     G.nodes[node][\"label\"] = str(node)\n",
    "#     G.nodes[node][\"title\"] = f\"Feature: {x[node].tolist()}\"\n",
    "#     G.nodes[node][\"color\"] = \"green\" if node < 26 else \"blue\"\n",
    "\n",
    "# # Create Pyvis graph\n",
    "# net = Network(height='600px', width='100%', directed=True, notebook=True)\n",
    "# net.from_nx(G)\n",
    "# net.save_graph(\"incidence_multigraph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "899467ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connected_subgraphs(G, k, n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    if G.number_of_nodes() <= k:\n",
    "        raise ValueError(\"Cannot remove more nodes than exist in the graph.\")\n",
    "\n",
    "    subgraphs = []\n",
    "    attempts = 0\n",
    "    max_attempts = 100 * n  # safety to avoid infinite loops\n",
    "\n",
    "    while len(subgraphs) < n and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        nodes_to_remove = random.sample(list(G.nodes()), k)\n",
    "        G_sub = G.copy()\n",
    "        G_sub.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "        if nx.is_weakly_connected(G_sub):\n",
    "            subgraphs.append(G_sub)\n",
    "\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "763861cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyg_data_to_nx_multigraph(data):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # Step 1: Add all nodes with features\n",
    "    for i in range(data.num_nodes):\n",
    "        G.add_node(i, x=data.x[i].tolist())  # attach node features\n",
    "\n",
    "    # Step 2: Add all edges (with support for multiple edges)\n",
    "    edge_list = data.edge_index.t().tolist()\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    return G\n",
    "G = pyg_data_to_nx_multigraph(data=data_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "599af526",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_obj_ls = []\n",
    "subgraph_ls = []\n",
    "for k in range(5):\n",
    "    subgraphs = generate_connected_subgraphs(G, k, n=10, seed=123)\n",
    "    subgraph_ls.extend(subgraphs)\n",
    "\n",
    "for nx_graph in subgraph_ls:\n",
    "    # Get all edges with duplicates preserved\n",
    "    edge_list = [(u, v) for u, v, _ in nx_graph.edges(keys=True)]\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Build identity features using original node indices\n",
    "    all_nodes = list(nx_graph.nodes())\n",
    "    max_node_id = max(all_nodes)\n",
    "    x = torch.eye(max_node_id + 1)  # size = [max_node_id + 1, max_node_id + 1]\n",
    "\n",
    "    # Some nodes might be missing → subset x to only the active node set\n",
    "    node_mask = torch.zeros_like(x)\n",
    "    for node in all_nodes:\n",
    "        node_mask[node] = x[node]\n",
    "    x_subset = node_mask  # shape = [max_node_id + 1, feature_dim]\n",
    "\n",
    "    data = Data(x=x_subset, edge_index=edge_index)\n",
    "    graph_data_obj_ls.append(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdff2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_data_obj_ls = []\n",
    "\n",
    "for data in graph_data_obj_ls:\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    masked_graphs_per_data = []  # inner list for each data graph\n",
    "\n",
    "    for edges_to_remove in range(1, 6):  # from 1 to 5\n",
    "        for _ in range(15):  # generate 15 graphs per mask level\n",
    "            if num_edges <= edges_to_remove:\n",
    "                continue  # can't remove more edges than exist\n",
    "\n",
    "            data_copy = copy.deepcopy(data)\n",
    "            edge_indices = list(range(num_edges))\n",
    "            to_remove = random.sample(edge_indices, edges_to_remove)\n",
    "\n",
    "            mask = torch.ones(num_edges, dtype=torch.bool)\n",
    "            mask[to_remove] = False\n",
    "\n",
    "            data_copy.edge_index = data.edge_index[:, mask]\n",
    "\n",
    "            if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "                data_copy.edge_attr = data.edge_attr[mask]\n",
    "\n",
    "            masked_graphs_per_data.append(data_copy)\n",
    "\n",
    "    subgraph_data_obj_ls.append(masked_graphs_per_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecdfcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "TOTAL_NODES = 45  # Size of node space (from G)\n",
    "HIDDEN_DIM1 = 64\n",
    "HIDDEN_DIM2 = 128\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class EdgeDecoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels * 2, 1)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        edge_feats = torch.cat([z[src], z[dst]], dim=1)\n",
    "        return self.linear(edge_feats).squeeze()\n",
    "\n",
    "class GraphCompletionModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GCNEncoder(in_channels, hidden_channels)\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, candidate_edges):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        scores = self.decoder(z, candidate_edges)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "25e88616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_non_edges(num_nodes, existing_edges, num_samples):\n",
    "    existing_set = set(existing_edges)\n",
    "    all_possible = [(i, j) for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "    candidates = list(set(all_possible) - existing_set)\n",
    "    return random.sample(candidates, min(num_samples, len(candidates)))\n",
    "\n",
    "def compute_accuracy(scores, labels, threshold=0.5):\n",
    "    preds = (torch.sigmoid(scores) > threshold).float()\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f68da165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_supervised_data(G_prime_list, G_double_prime_LOL, total_nodes):\n",
    "    data = []\n",
    "    for i in range(len(G_prime_list)):\n",
    "        G_prime = G_prime_list[i]\n",
    "        G_double_primes = G_double_prime_LOL[i]\n",
    "\n",
    "        true_edges = list(map(tuple, G_prime.edge_index.t().tolist()))\n",
    "\n",
    "        for G_double_prime in G_double_primes:\n",
    "            observed_edges = list(map(tuple, G_double_prime.edge_index.t().tolist()))\n",
    "            positive_edges = [e for e in true_edges if e not in observed_edges]\n",
    "            negative_edges = sample_non_edges(total_nodes, true_edges, len(positive_edges))\n",
    "\n",
    "            data.append((G_double_prime, positive_edges, negative_edges))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9f9df52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, total_nodes, epochs=20, lr=0.01, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for G_double_prime, pos_edges, neg_edges in train_data:\n",
    "            x = torch.eye(total_nodes).to(device)\n",
    "            edge_index = G_double_prime.edge_index.to(device)\n",
    "            candidate_edges = torch.tensor(pos_edges + neg_edges, dtype=torch.long).t().contiguous().to(device)\n",
    "            labels = torch.tensor([1]*len(pos_edges) + [0]*len(neg_edges), dtype=torch.float).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(x, edge_index, candidate_edges)\n",
    "            loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            total_val_acc = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for G_double_prime, pos_edges, neg_edges in test_data:\n",
    "                x = torch.eye(total_nodes).to(device)\n",
    "                edge_index = G_double_prime.edge_index.to(device)\n",
    "                \n",
    "                candidate_edges = torch.tensor(pos_edges + neg_edges, dtype=torch.long).t().contiguous().to(device)\n",
    "                labels = torch.tensor([1]*len(pos_edges) + [0]*len(neg_edges), dtype=torch.float).to(device)\n",
    "\n",
    "                scores = model(x, edge_index, candidate_edges)\n",
    "                probs = torch.sigmoid(scores)\n",
    "                val_loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "                total_val_acc += compute_accuracy(scores, labels) * len(labels)\n",
    "                total_samples += len(labels)\n",
    "        \n",
    "    \n",
    "\n",
    "            print(f\"[Epoch {epoch+1}] Train Loss: {total_loss:.4f} | Val Loss: {total_val_loss:.4f} | Val Acc: {total_val_acc/total_samples:.4f}\")\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ac5f17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(G, G_prime_list, G_double_prime_LOL):\n",
    "    data = prepare_supervised_data(G_prime_list, G_double_prime_LOL, TOTAL_NODES)\n",
    "    train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = GraphCompletionModel(in_channels=TOTAL_NODES, hidden_channels=HIDDEN_DIM1)\n",
    "    probs = train_model(model, train_set, test_set, TOTAL_NODES, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "\n",
    "    return model, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "371e2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1240.6079 | Val Loss: 246.5785 | Val Acc: 0.8686\n",
      "[Epoch 2] Train Loss: 952.8465 | Val Loss: 231.6220 | Val Acc: 0.8860\n",
      "[Epoch 3] Train Loss: 865.8727 | Val Loss: 214.6990 | Val Acc: 0.8896\n",
      "[Epoch 4] Train Loss: 824.5839 | Val Loss: 201.2178 | Val Acc: 0.8958\n",
      "[Epoch 5] Train Loss: 789.7206 | Val Loss: 198.7079 | Val Acc: 0.9039\n",
      "[Epoch 6] Train Loss: 774.1289 | Val Loss: 190.4717 | Val Acc: 0.9083\n",
      "[Epoch 7] Train Loss: 756.7518 | Val Loss: 189.9522 | Val Acc: 0.9013\n",
      "[Epoch 8] Train Loss: 743.4630 | Val Loss: 192.5995 | Val Acc: 0.9053\n",
      "[Epoch 9] Train Loss: 732.1203 | Val Loss: 189.3522 | Val Acc: 0.9037\n",
      "[Epoch 10] Train Loss: 721.3701 | Val Loss: 193.3941 | Val Acc: 0.8964\n",
      "[Epoch 11] Train Loss: 713.8278 | Val Loss: 181.1788 | Val Acc: 0.9075\n",
      "[Epoch 12] Train Loss: 710.9090 | Val Loss: 181.4613 | Val Acc: 0.9139\n",
      "[Epoch 13] Train Loss: 699.8323 | Val Loss: 185.0325 | Val Acc: 0.9044\n",
      "[Epoch 14] Train Loss: 702.6161 | Val Loss: 183.5876 | Val Acc: 0.9143\n",
      "[Epoch 15] Train Loss: 696.2634 | Val Loss: 179.3910 | Val Acc: 0.9183\n",
      "[Epoch 16] Train Loss: 687.9012 | Val Loss: 180.7609 | Val Acc: 0.9201\n",
      "[Epoch 17] Train Loss: 685.3024 | Val Loss: 176.0216 | Val Acc: 0.9194\n",
      "[Epoch 18] Train Loss: 685.3760 | Val Loss: 179.5954 | Val Acc: 0.9178\n",
      "[Epoch 19] Train Loss: 681.1596 | Val Loss: 175.4305 | Val Acc: 0.9192\n",
      "[Epoch 20] Train Loss: 683.7602 | Val Loss: 168.5840 | Val Acc: 0.9227\n",
      "[Epoch 21] Train Loss: 675.9513 | Val Loss: 173.9058 | Val Acc: 0.9201\n",
      "[Epoch 22] Train Loss: 679.7494 | Val Loss: 168.8025 | Val Acc: 0.9209\n",
      "[Epoch 23] Train Loss: 672.0072 | Val Loss: 170.1624 | Val Acc: 0.9238\n",
      "[Epoch 24] Train Loss: 673.7913 | Val Loss: 173.3008 | Val Acc: 0.9225\n",
      "[Epoch 25] Train Loss: 672.8899 | Val Loss: 173.0987 | Val Acc: 0.9218\n",
      "[Epoch 26] Train Loss: 673.8996 | Val Loss: 171.2087 | Val Acc: 0.9225\n",
      "[Epoch 27] Train Loss: 671.2120 | Val Loss: 172.0878 | Val Acc: 0.9223\n",
      "[Epoch 28] Train Loss: 669.1955 | Val Loss: 174.2266 | Val Acc: 0.9172\n",
      "[Epoch 29] Train Loss: 663.2944 | Val Loss: 166.9035 | Val Acc: 0.9234\n",
      "[Epoch 30] Train Loss: 664.3959 | Val Loss: 171.5997 | Val Acc: 0.9234\n",
      "[Epoch 31] Train Loss: 667.8940 | Val Loss: 167.1563 | Val Acc: 0.9240\n",
      "[Epoch 32] Train Loss: 665.1105 | Val Loss: 167.6182 | Val Acc: 0.9236\n",
      "[Epoch 33] Train Loss: 659.0062 | Val Loss: 167.3735 | Val Acc: 0.9258\n",
      "[Epoch 34] Train Loss: 658.8970 | Val Loss: 176.2884 | Val Acc: 0.9216\n",
      "[Epoch 35] Train Loss: 653.8320 | Val Loss: 170.7177 | Val Acc: 0.9267\n",
      "[Epoch 36] Train Loss: 654.4998 | Val Loss: 168.8073 | Val Acc: 0.9251\n",
      "[Epoch 37] Train Loss: 650.8596 | Val Loss: 166.8937 | Val Acc: 0.9280\n",
      "[Epoch 38] Train Loss: 652.6814 | Val Loss: 169.7508 | Val Acc: 0.9245\n",
      "[Epoch 39] Train Loss: 654.7536 | Val Loss: 165.4688 | Val Acc: 0.9265\n",
      "[Epoch 40] Train Loss: 654.5562 | Val Loss: 167.8295 | Val Acc: 0.9225\n",
      "[Epoch 41] Train Loss: 651.0253 | Val Loss: 164.0049 | Val Acc: 0.9287\n",
      "[Epoch 42] Train Loss: 649.7482 | Val Loss: 169.4656 | Val Acc: 0.9280\n",
      "[Epoch 43] Train Loss: 652.9909 | Val Loss: 169.1029 | Val Acc: 0.9273\n",
      "[Epoch 44] Train Loss: 648.4283 | Val Loss: 171.3024 | Val Acc: 0.9240\n",
      "[Epoch 45] Train Loss: 651.6605 | Val Loss: 168.8104 | Val Acc: 0.9240\n",
      "[Epoch 46] Train Loss: 649.2951 | Val Loss: 169.1765 | Val Acc: 0.9229\n",
      "[Epoch 47] Train Loss: 644.7729 | Val Loss: 169.4529 | Val Acc: 0.9284\n",
      "[Epoch 48] Train Loss: 641.0637 | Val Loss: 164.8363 | Val Acc: 0.9273\n",
      "[Epoch 49] Train Loss: 648.2708 | Val Loss: 171.1583 | Val Acc: 0.9249\n",
      "[Epoch 50] Train Loss: 642.7011 | Val Loss: 168.9653 | Val Acc: 0.9284\n"
     ]
    }
   ],
   "source": [
    "model, probs = run_pipeline(G=G, G_prime_list=graph_data_obj_ls, G_double_prime_LOL=subgraph_data_obj_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fb4ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def evaluate_graph_completion(model, G_prime_list, G_double_prime_LOL, total_nodes=45, threshold=0.5, device='cpu'):\n",
    "    all_results = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, G_prime in enumerate(G_prime_list):\n",
    "        edges_G_prime = set(map(tuple, G_prime.edge_index.t().tolist()))\n",
    "\n",
    "        for G_double_prime in G_double_prime_LOL[i]:\n",
    "            edges_G_double_prime = set(map(tuple, G_double_prime.edge_index.t().tolist()))\n",
    "\n",
    "            # 1. Ground-truth missing edges (positive class)\n",
    "            true_missing_edges = list(edges_G_prime - edges_G_double_prime)\n",
    "\n",
    "            # 2. Create candidate edges (all possible, excluding existing G″ edges)\n",
    "            candidate_edges = [\n",
    "                (u, v) for u in range(total_nodes) for v in range(total_nodes)\n",
    "                if u != v and (u, v) not in edges_G_double_prime\n",
    "            ]\n",
    "            candidate_edges_tensor = torch.tensor(candidate_edges, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "            # 3. Get model prediction scores\n",
    "            x = torch.eye(total_nodes).to(device)\n",
    "            edge_index = G_double_prime.edge_index.to(device)\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(model(x, edge_index, candidate_edges_tensor)).cpu()\n",
    "\n",
    "            # 4. Thresholding\n",
    "            predicted_edges = [\n",
    "                candidate_edges[i] for i, p in enumerate(probs) if p > threshold\n",
    "            ]\n",
    "\n",
    "            # 5. Classification\n",
    "            y_true = [1 if e in true_missing_edges else 0 for e in candidate_edges]\n",
    "            y_pred = [1 if p > threshold else 0 for p in probs]\n",
    "\n",
    "            # 6. Compute metrics\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "            # 7. Save per-G″ result\n",
    "            result = {\n",
    "                'G_index': i,\n",
    "                'correct_predictions': [e for e in predicted_edges if e in true_missing_edges],\n",
    "                'incorrect_predictions': [e for e in predicted_edges if e not in true_missing_edges],\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'accuracy': acc,\n",
    "                'num_predicted': len(predicted_edges),\n",
    "                'num_true_missing': len(true_missing_edges)\n",
    "            }\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "all_results = evaluate_graph_completion(model, graph_data_obj_ls, subgraph_data_obj_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73d6668d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G_index': 13,\n",
       " 'correct_predictions': [(6, 1), (22, 21)],\n",
       " 'incorrect_predictions': [(0, 1),\n",
       "  (0, 2),\n",
       "  (0, 6),\n",
       "  (0, 16),\n",
       "  (0, 17),\n",
       "  (0, 19),\n",
       "  (0, 20),\n",
       "  (0, 21),\n",
       "  (0, 23),\n",
       "  (0, 24),\n",
       "  (0, 34),\n",
       "  (0, 35),\n",
       "  (1, 2),\n",
       "  (1, 20),\n",
       "  (1, 21),\n",
       "  (2, 1),\n",
       "  (2, 21),\n",
       "  (3, 1),\n",
       "  (3, 2),\n",
       "  (3, 20),\n",
       "  (3, 21),\n",
       "  (4, 1),\n",
       "  (4, 2),\n",
       "  (4, 20),\n",
       "  (4, 21),\n",
       "  (5, 0),\n",
       "  (5, 1),\n",
       "  (5, 2),\n",
       "  (5, 16),\n",
       "  (5, 17),\n",
       "  (5, 19),\n",
       "  (5, 20),\n",
       "  (5, 21),\n",
       "  (5, 23),\n",
       "  (5, 34),\n",
       "  (5, 35),\n",
       "  (6, 2),\n",
       "  (6, 20),\n",
       "  (6, 21),\n",
       "  (7, 1),\n",
       "  (7, 20),\n",
       "  (7, 21),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 20),\n",
       "  (8, 21),\n",
       "  (9, 1),\n",
       "  (9, 20),\n",
       "  (9, 21),\n",
       "  (10, 1),\n",
       "  (10, 2),\n",
       "  (10, 20),\n",
       "  (10, 21),\n",
       "  (11, 1),\n",
       "  (11, 2),\n",
       "  (11, 20),\n",
       "  (11, 21),\n",
       "  (12, 1),\n",
       "  (12, 2),\n",
       "  (12, 20),\n",
       "  (12, 21),\n",
       "  (13, 1),\n",
       "  (13, 2),\n",
       "  (13, 20),\n",
       "  (13, 21),\n",
       "  (14, 1),\n",
       "  (14, 2),\n",
       "  (14, 20),\n",
       "  (14, 21),\n",
       "  (14, 23),\n",
       "  (15, 1),\n",
       "  (15, 2),\n",
       "  (15, 20),\n",
       "  (15, 21),\n",
       "  (16, 1),\n",
       "  (16, 2),\n",
       "  (16, 20),\n",
       "  (16, 21),\n",
       "  (17, 1),\n",
       "  (17, 2),\n",
       "  (17, 20),\n",
       "  (17, 21),\n",
       "  (18, 1),\n",
       "  (18, 2),\n",
       "  (18, 20),\n",
       "  (18, 21),\n",
       "  (19, 1),\n",
       "  (19, 2),\n",
       "  (19, 20),\n",
       "  (19, 21),\n",
       "  (20, 1),\n",
       "  (20, 2),\n",
       "  (20, 21),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (22, 2),\n",
       "  (22, 20),\n",
       "  (23, 1),\n",
       "  (23, 20),\n",
       "  (23, 21),\n",
       "  (24, 1),\n",
       "  (24, 20),\n",
       "  (24, 21),\n",
       "  (25, 1),\n",
       "  (25, 2),\n",
       "  (25, 20),\n",
       "  (25, 21),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (28, 21),\n",
       "  (30, 1),\n",
       "  (30, 2),\n",
       "  (30, 20),\n",
       "  (30, 21),\n",
       "  (31, 1),\n",
       "  (31, 2),\n",
       "  (31, 20),\n",
       "  (31, 21),\n",
       "  (32, 1),\n",
       "  (32, 21),\n",
       "  (33, 1),\n",
       "  (33, 2),\n",
       "  (33, 20),\n",
       "  (33, 21),\n",
       "  (34, 1),\n",
       "  (34, 2),\n",
       "  (34, 20),\n",
       "  (34, 21),\n",
       "  (35, 1),\n",
       "  (35, 2),\n",
       "  (35, 20),\n",
       "  (35, 21),\n",
       "  (36, 1),\n",
       "  (36, 21),\n",
       "  (37, 1),\n",
       "  (37, 2),\n",
       "  (37, 20),\n",
       "  (37, 21),\n",
       "  (38, 1),\n",
       "  (38, 2),\n",
       "  (38, 20),\n",
       "  (38, 21),\n",
       "  (39, 1),\n",
       "  (39, 20),\n",
       "  (40, 1),\n",
       "  (40, 2),\n",
       "  (40, 20),\n",
       "  (40, 21),\n",
       "  (40, 23),\n",
       "  (41, 1),\n",
       "  (41, 20),\n",
       "  (41, 21),\n",
       "  (42, 1),\n",
       "  (42, 2),\n",
       "  (42, 20),\n",
       "  (42, 21),\n",
       "  (43, 1),\n",
       "  (43, 2),\n",
       "  (43, 20),\n",
       "  (43, 21),\n",
       "  (44, 1),\n",
       "  (44, 21)],\n",
       " 'precision': 0.012121212121212121,\n",
       " 'recall': 1.0,\n",
       " 'f1_score': 0.023952095808383235,\n",
       " 'accuracy': 0.9157187176835574,\n",
       " 'num_predicted': 165,\n",
       " 'num_true_missing': 2}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b2fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
