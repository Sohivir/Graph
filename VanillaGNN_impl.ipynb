{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abae7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soham\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e5e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a71c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------\n",
    "TOTAL_NODES = 26  # Size of node space (from G)\n",
    "HIDDEN_DIM = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class EdgeDecoder(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels * 2, 1)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        src, dst = edge_index\n",
    "        edge_feats = torch.cat([z[src], z[dst]], dim=1)\n",
    "        return self.linear(edge_feats).squeeze()\n",
    "\n",
    "class GraphCompletionModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GCNEncoder(in_channels, hidden_channels)\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, candidate_edges):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        scores = self.decoder(z, candidate_edges)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32a61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_non_edges(num_nodes, existing_edges, num_samples):\n",
    "    existing_set = set(existing_edges)\n",
    "    all_possible = [(i, j) for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "    candidates = list(set(all_possible) - existing_set)\n",
    "    return random.sample(candidates, min(num_samples, len(candidates)))\n",
    "\n",
    "def compute_accuracy(scores, labels, threshold=0.5):\n",
    "    preds = (torch.sigmoid(scores) > threshold).float()\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ed415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_supervised_data(G_prime_list, G_double_prime_LOL, total_nodes):\n",
    "    data = []\n",
    "    for i in range(len(G_prime_list)):\n",
    "        G_prime = G_prime_list[i]\n",
    "        G_double_primes = G_double_prime_LOL[i]\n",
    "\n",
    "        true_edges = list(map(tuple, G_prime.edge_index.t().tolist()))\n",
    "\n",
    "        for G_double_prime in G_double_primes:\n",
    "            observed_edges = list(map(tuple, G_double_prime.edge_index.t().tolist()))\n",
    "            positive_edges = [e for e in true_edges if e not in observed_edges]\n",
    "            negative_edges = sample_non_edges(total_nodes, true_edges, len(positive_edges))\n",
    "\n",
    "            data.append((G_double_prime, positive_edges, negative_edges))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df967a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, total_nodes, epochs=50, lr=0.01, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for G_double_prime, pos_edges, neg_edges in train_data:\n",
    "            x = torch.eye(total_nodes).to(device)\n",
    "            edge_index = G_double_prime.edge_index.to(device)\n",
    "            candidate_edges = torch.tensor(pos_edges + neg_edges, dtype=torch.long).t().contiguous().to(device)\n",
    "            labels = torch.tensor([1]*len(pos_edges) + [0]*len(neg_edges), dtype=torch.float).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(x, edge_index, candidate_edges)\n",
    "            loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            total_val_acc = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for G_double_prime, pos_edges, neg_edges in test_data:\n",
    "                x = torch.eye(total_nodes).to(device)\n",
    "                edge_index = G_double_prime.edge_index.to(device)\n",
    "                candidate_edges = torch.tensor(pos_edges + neg_edges, dtype=torch.long).t().contiguous().to(device)\n",
    "                labels = torch.tensor([1]*len(pos_edges) + [0]*len(neg_edges), dtype=torch.float).to(device)\n",
    "\n",
    "                scores = model(x, edge_index, candidate_edges)\n",
    "                val_loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "                total_val_acc += compute_accuracy(scores, labels) * len(labels)\n",
    "                total_samples += len(labels)\n",
    "\n",
    "            print(f\"[Epoch {epoch+1}] Train Loss: {total_loss:.4f} | Val Loss: {total_val_loss:.4f} | Val Acc: {total_val_acc/total_samples:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c841a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pipeline(G, G_prime_list, G_double_prime_LOL):\n",
    "    data = prepare_supervised_data(G_prime_list, G_double_prime_LOL, TOTAL_NODES)\n",
    "    train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = GraphCompletionModel(in_channels=TOTAL_NODES, hidden_channels=HIDDEN_DIM)\n",
    "    train_model(model, train_set, test_set, TOTAL_NODES, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be382bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"Main_graph_withNodeFeats.pkl\", \"rb\") as f:\n",
    "    G = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e2f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connected_subgraphs(G, k, n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    if G.number_of_nodes() <= k:\n",
    "        raise ValueError(\"Cannot remove more nodes than exist in the graph.\")\n",
    "\n",
    "    subgraphs = []\n",
    "    attempts = 0\n",
    "    max_attempts = 100 * n  # safety to avoid infinite loops\n",
    "\n",
    "    while len(subgraphs) < n and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        nodes_to_remove = random.sample(list(G.nodes()), k)\n",
    "        G_sub = G.copy()\n",
    "        G_sub.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "        if nx.is_weakly_connected(G_sub):\n",
    "            subgraphs.append(G_sub)\n",
    "\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f18f1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ls = []\n",
    "subgraph_ls = []\n",
    "for k in range(5):\n",
    "    subgraphs= generate_connected_subgraphs(G, k, n=10, seed=123)\n",
    "    subgraph_ls.append(subgraphs)\n",
    "\n",
    "subgraph_ls = [g for graphs in subgraph_ls for g in graphs]\n",
    "graph_data_obj_ls = []\n",
    "for nx_graph in subgraph_ls:\n",
    "    graph_data_obj = from_networkx(nx_graph)\n",
    "    graph_data_obj_ls.append(graph_data_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce002b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_data_obj_ls = []\n",
    "\n",
    "for data in graph_data_obj_ls:\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    masked_graphs_per_data = []  # inner list for each data graph\n",
    "\n",
    "    for edges_to_remove in range(1, 6):  # from 1 to 5\n",
    "        for _ in range(15):  # generate 15 graphs per mask level\n",
    "            if num_edges <= edges_to_remove:\n",
    "                continue  # can't remove more edges than exist\n",
    "\n",
    "            data_copy = copy.deepcopy(data)\n",
    "            edge_indices = list(range(num_edges))\n",
    "            to_remove = random.sample(edge_indices, edges_to_remove)\n",
    "\n",
    "            mask = torch.ones(num_edges, dtype=torch.bool)\n",
    "            mask[to_remove] = False\n",
    "\n",
    "            data_copy.edge_index = data.edge_index[:, mask]\n",
    "\n",
    "            if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "                data_copy.edge_attr = data.edge_attr[mask]\n",
    "\n",
    "            masked_graphs_per_data.append(data_copy)\n",
    "\n",
    "    subgraph_data_obj_ls.append(masked_graphs_per_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2323f3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1624.5221 | Val Loss: 379.9838 | Val Acc: 0.7449\n",
      "[Epoch 2] Train Loss: 1478.9567 | Val Loss: 357.8744 | Val Acc: 0.7705\n",
      "[Epoch 3] Train Loss: 1430.2219 | Val Loss: 345.0938 | Val Acc: 0.7827\n",
      "[Epoch 4] Train Loss: 1399.7146 | Val Loss: 339.1912 | Val Acc: 0.7984\n",
      "[Epoch 5] Train Loss: 1380.0853 | Val Loss: 332.1214 | Val Acc: 0.8034\n",
      "[Epoch 6] Train Loss: 1364.8964 | Val Loss: 329.9406 | Val Acc: 0.8087\n",
      "[Epoch 7] Train Loss: 1354.4553 | Val Loss: 324.6808 | Val Acc: 0.8110\n",
      "[Epoch 8] Train Loss: 1343.8439 | Val Loss: 325.0956 | Val Acc: 0.8076\n",
      "[Epoch 9] Train Loss: 1335.9669 | Val Loss: 323.9693 | Val Acc: 0.8138\n",
      "[Epoch 10] Train Loss: 1331.9361 | Val Loss: 323.1970 | Val Acc: 0.8105\n",
      "[Epoch 11] Train Loss: 1326.0163 | Val Loss: 323.3902 | Val Acc: 0.8039\n",
      "[Epoch 12] Train Loss: 1318.2781 | Val Loss: 327.3375 | Val Acc: 0.8094\n",
      "[Epoch 13] Train Loss: 1315.2394 | Val Loss: 326.5176 | Val Acc: 0.8079\n",
      "[Epoch 14] Train Loss: 1310.7018 | Val Loss: 328.9232 | Val Acc: 0.8110\n",
      "[Epoch 15] Train Loss: 1310.0178 | Val Loss: 328.0592 | Val Acc: 0.8158\n",
      "[Epoch 16] Train Loss: 1303.5502 | Val Loss: 319.7890 | Val Acc: 0.8156\n",
      "[Epoch 17] Train Loss: 1301.2683 | Val Loss: 322.2923 | Val Acc: 0.8094\n",
      "[Epoch 18] Train Loss: 1295.8084 | Val Loss: 321.5903 | Val Acc: 0.8160\n",
      "[Epoch 19] Train Loss: 1291.7814 | Val Loss: 324.8382 | Val Acc: 0.8112\n",
      "[Epoch 20] Train Loss: 1288.6033 | Val Loss: 314.3673 | Val Acc: 0.8101\n",
      "[Epoch 21] Train Loss: 1284.2344 | Val Loss: 312.2225 | Val Acc: 0.8174\n",
      "[Epoch 22] Train Loss: 1279.3070 | Val Loss: 309.6676 | Val Acc: 0.8136\n",
      "[Epoch 23] Train Loss: 1274.8429 | Val Loss: 307.7105 | Val Acc: 0.8114\n",
      "[Epoch 24] Train Loss: 1273.1446 | Val Loss: 307.5824 | Val Acc: 0.8132\n",
      "[Epoch 25] Train Loss: 1271.4284 | Val Loss: 311.6683 | Val Acc: 0.8068\n",
      "[Epoch 26] Train Loss: 1267.3959 | Val Loss: 309.3504 | Val Acc: 0.8026\n",
      "[Epoch 27] Train Loss: 1264.7777 | Val Loss: 308.3520 | Val Acc: 0.8121\n",
      "[Epoch 28] Train Loss: 1264.5273 | Val Loss: 308.0391 | Val Acc: 0.8121\n",
      "[Epoch 29] Train Loss: 1263.8505 | Val Loss: 306.5411 | Val Acc: 0.8140\n",
      "[Epoch 30] Train Loss: 1263.6495 | Val Loss: 308.7313 | Val Acc: 0.7977\n",
      "[Epoch 31] Train Loss: 1261.1560 | Val Loss: 310.2856 | Val Acc: 0.8030\n",
      "[Epoch 32] Train Loss: 1259.7745 | Val Loss: 311.5384 | Val Acc: 0.8061\n",
      "[Epoch 33] Train Loss: 1257.0538 | Val Loss: 310.7783 | Val Acc: 0.8087\n",
      "[Epoch 34] Train Loss: 1258.0501 | Val Loss: 311.9927 | Val Acc: 0.8061\n",
      "[Epoch 35] Train Loss: 1256.2080 | Val Loss: 310.9459 | Val Acc: 0.8034\n",
      "[Epoch 36] Train Loss: 1254.2506 | Val Loss: 310.3657 | Val Acc: 0.8052\n",
      "[Epoch 37] Train Loss: 1254.9525 | Val Loss: 309.3536 | Val Acc: 0.8054\n",
      "[Epoch 38] Train Loss: 1254.0828 | Val Loss: 311.0269 | Val Acc: 0.8076\n",
      "[Epoch 39] Train Loss: 1252.4572 | Val Loss: 311.8284 | Val Acc: 0.8045\n",
      "[Epoch 40] Train Loss: 1253.0885 | Val Loss: 312.7705 | Val Acc: 0.7966\n",
      "[Epoch 41] Train Loss: 1251.0716 | Val Loss: 313.6624 | Val Acc: 0.8008\n",
      "[Epoch 42] Train Loss: 1252.3338 | Val Loss: 311.9720 | Val Acc: 0.7966\n",
      "[Epoch 43] Train Loss: 1250.8839 | Val Loss: 310.8548 | Val Acc: 0.7984\n",
      "[Epoch 44] Train Loss: 1249.6634 | Val Loss: 311.0786 | Val Acc: 0.7986\n",
      "[Epoch 45] Train Loss: 1247.9169 | Val Loss: 312.4247 | Val Acc: 0.7968\n",
      "[Epoch 46] Train Loss: 1248.1890 | Val Loss: 311.2676 | Val Acc: 0.8019\n",
      "[Epoch 47] Train Loss: 1247.6085 | Val Loss: 315.6026 | Val Acc: 0.7944\n",
      "[Epoch 48] Train Loss: 1247.0567 | Val Loss: 313.3571 | Val Acc: 0.7964\n",
      "[Epoch 49] Train Loss: 1247.4366 | Val Loss: 311.8130 | Val Acc: 0.7995\n",
      "[Epoch 50] Train Loss: 1246.9675 | Val Loss: 309.4596 | Val Acc: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphCompletionModel(\n",
       "  (encoder): GCNEncoder(\n",
       "    (conv1): GCNConv(26, 64)\n",
       "    (conv2): GCNConv(64, 64)\n",
       "  )\n",
       "  (decoder): EdgeDecoder(\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline(G=G, G_prime_list=graph_data_obj_ls, G_double_prime_LOL=subgraph_data_obj_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500a3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
