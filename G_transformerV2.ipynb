{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b97b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soham\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.sparse import csgraph\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import random\n",
    "import copy\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd499f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedGraphDataset(Dataset):\n",
    "    def __init__(self, graph_data_obj_ls, subgraph_data_obj_ls):\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        for full_graph, masked_versions in zip(graph_data_obj_ls, subgraph_data_obj_ls):\n",
    "            for masked_graph in masked_versions:\n",
    "                self.inputs.append(masked_graph)  # G''\n",
    "                self.targets.append(full_graph)   # G'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_graph = self.inputs[idx]\n",
    "        target_graph = self.targets[idx]\n",
    "\n",
    "        # Ensure that all tensors are returned\n",
    "        return {\n",
    "            'input': input_graph,\n",
    "            'target': target_graph\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16a80e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inc_matrix_aug = np.loadtxt(\"Aug_inc_matrix\")\n",
    "inc_matrix_aug = inc_matrix_aug.reshape(-1,50)\n",
    "\n",
    "num_nodes, num_edges = inc_matrix_aug.shape\n",
    "\n",
    "# --- Step 2: Convert to edge_index for PyG (multi-edges allowed) ---\n",
    "edge_list = []\n",
    "for j in range(num_edges):\n",
    "    col = inc_matrix_aug[:, j]\n",
    "    src = np.where(col == -1)[0]\n",
    "    dst = np.where(col == 1)[0]\n",
    "    if len(src) == 1 and len(dst) == 1:\n",
    "        edge_list.append((src[0], dst[0]))  # directed edge\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "x = torch.eye(45, dtype=torch.float)\n",
    "\n",
    "# --- Step 3: Create PyG Data object ---\n",
    "data_inp= Data(x=x, edge_index=edge_index, )\n",
    "\n",
    "\n",
    "def pyg_data_to_nx_multigraph(data):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # Step 1: Add all nodes with features\n",
    "    for i in range(data.num_nodes):\n",
    "        G.add_node(i, x=data.x[i].tolist())  # attach node features\n",
    "\n",
    "    # Step 2: Add all edges (with support for multiple edges)\n",
    "    edge_list = data.edge_index.t().tolist()\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    return G\n",
    "G = pyg_data_to_nx_multigraph(data=data_inp)\n",
    "def generate_connected_subgraphs(G, k, n, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    if G.number_of_nodes() <= k:\n",
    "        raise ValueError(\"Cannot remove more nodes than exist in the graph.\")\n",
    "\n",
    "    subgraphs = []\n",
    "    attempts = 0\n",
    "    max_attempts = 100 * n  # safety to avoid infinite loops\n",
    "\n",
    "    while len(subgraphs) < n and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        nodes_to_remove = random.sample(list(G.nodes()), k)\n",
    "        G_sub = G.copy()\n",
    "        G_sub.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "        if nx.is_weakly_connected(G_sub):\n",
    "            subgraphs.append(G_sub)\n",
    "\n",
    "    return subgraphs\n",
    "graph_data_obj_ls = []\n",
    "subgraph_ls = []\n",
    "for k in range(5):\n",
    "    subgraphs = generate_connected_subgraphs(G, k, n=10, seed=123)\n",
    "    subgraph_ls.extend(subgraphs)\n",
    "\n",
    "for nx_graph in subgraph_ls:\n",
    "    # Get all edges with duplicates preserved\n",
    "    edge_list = [(u, v) for u, v, _ in nx_graph.edges(keys=True)]\n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Build identity features using original node indices\n",
    "    all_nodes = list(nx_graph.nodes())\n",
    "    num_nodes_global = 45\n",
    "    x = torch.eye(num_nodes_global)  # [45, 45]\n",
    "    node_mask = torch.zeros_like(x)  # [45, 45]\n",
    "\n",
    "    for node in all_nodes:\n",
    "        node_mask[node] = x[node]  # Keep features only for nodes in this subgraph\n",
    "\n",
    "    x_subset = node_mask\n",
    "\n",
    "    data = Data(x=x_subset, edge_index=edge_index)\n",
    "    graph_data_obj_ls.append(data)\n",
    "subgraph_data_obj_ls = []\n",
    "\n",
    "for data in graph_data_obj_ls:\n",
    "    G_nx = to_networkx(data, to_undirected=False)\n",
    "    incidence_matrix = nx.incidence_matrix(G_nx, oriented=True).toarray()\n",
    "    rank = np.linalg.matrix_rank(incidence_matrix)\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    masked_graphs_per_data = []\n",
    "\n",
    "    for edges_to_remove in range(rank, min(rank + 6, num_edges)):\n",
    "        for _ in range(15):\n",
    "            if num_edges <= edges_to_remove:\n",
    "                continue\n",
    "\n",
    "            data_copy = copy.deepcopy(data)\n",
    "\n",
    "            # -------------------------------\n",
    "            # 1. Mask edges\n",
    "            edge_indices = list(range(num_edges))\n",
    "            to_remove = random.sample(edge_indices, edges_to_remove)\n",
    "            mask = torch.ones(num_edges, dtype=torch.bool)\n",
    "            mask[to_remove] = False\n",
    "            data_copy.edge_index = data.edge_index[:, mask]\n",
    "\n",
    "            if hasattr(data, 'edge_attr') and data.edge_attr is not None:\n",
    "                data_copy.edge_attr = data.edge_attr[mask]\n",
    "\n",
    "            # -------------------------------\n",
    "            # 2. Mask nodes (retain ~90% randomly)\n",
    "            node_mask = torch.ones(45, dtype=torch.bool)\n",
    "            total_nodes = 45\n",
    "            num_nodes_to_mask = int(0.1 * total_nodes)\n",
    "            nodes_to_mask = random.sample(range(45), num_nodes_to_mask)\n",
    "            node_mask[nodes_to_mask] = False\n",
    "\n",
    "            data_copy.masked_nodes = node_mask  # Add this attribute to use later\n",
    "\n",
    "            masked_graphs_per_data.append(data_copy)\n",
    "\n",
    "    subgraph_data_obj_ls.append(masked_graphs_per_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20b79deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = MaskedGraphDataset(graph_data_obj_ls, subgraph_data_obj_ls)\n",
    "\n",
    "# Split: 80% train, 20% validation\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# PyTorch Geometric uses a custom collate_fn\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "train_loader = PyGDataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = PyGDataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fe2bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, num_nodes=45, in_dim=45, hidden_dim=128, num_heads=4, num_layers=3):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_proj = nn.Linear(in_dim, hidden_dim)\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerConv(hidden_dim, hidden_dim // num_heads, heads=num_heads, dropout=0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.node_predictor = nn.Linear(hidden_dim, in_dim)  # Node classification\n",
    "        self.edge_predictor = nn.Bilinear(hidden_dim, hidden_dim, 1)  # Edge classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, edge_index).relu()\n",
    "\n",
    "        node_logits = None\n",
    "        if hasattr(data, 'masked_nodes'):\n",
    "            masked_nodes = data.masked_nodes.bool()\n",
    "            node_logits = self.node_predictor(x)\n",
    "\n",
    "        # Edge logits for all possible pairs (optional: restrict to sampled pairs in loss)\n",
    "        edge_logits_raw = torch.matmul(x, x.T)\n",
    "        edge_logits = torch.sigmoid(edge_logits_raw)\n",
    "\n",
    "        # Remove self-loop predictions by zeroing diagonal\n",
    "        edge_logits.fill_diagonal_(0)\n",
    "\n",
    "        return {\n",
    "            'node_logits': node_logits,\n",
    "            'edge_logits': edge_logits,\n",
    "            'edge_logits_raw': edge_logits_raw,\n",
    "            'final_node_embeddings': x\n",
    "        }\n",
    "\n",
    "def node_reconstruction_loss(output_logits, target_x, masked_nodes):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    target_classes = target_x.argmax(dim=1)\n",
    "    masked_indices = masked_nodes.nonzero(as_tuple=True)[0]\n",
    "    return loss_fn(output_logits[masked_indices], target_classes[masked_indices])\n",
    "\n",
    "\n",
    "def edge_reconstruction_loss(edge_logits_raw, edge_index, num_nodes):\n",
    "    # Ground truth adjacency matrix\n",
    "    adj_true = torch.zeros((num_nodes, num_nodes), device=edge_logits_raw.device)\n",
    "    adj_true[edge_index[0], edge_index[1]] = 1.0\n",
    "\n",
    "    # Flatten for BCEWithLogitsLoss\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "        edge_logits_raw.view(-1),\n",
    "        adj_true.view(-1)\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "\n",
    "def pad_to_full_graph(pred_adj, full_size=45):\n",
    "    \"\"\"\n",
    "    Pads a square predicted adjacency matrix to a full_size x full_size matrix.\n",
    "    \"\"\"\n",
    "    padded = torch.zeros(full_size, full_size, device=pred_adj.device)\n",
    "    size = pred_adj.size(0)\n",
    "    if size > full_size:\n",
    "        raise ValueError(f\"Predicted adjacency size {size} exceeds full size {full_size}\")\n",
    "    padded[:size, :size] = pred_adj\n",
    "    return padded\n",
    "\n",
    "def graph_edit_distance_loss(output, target, full_size=45):\n",
    "    \"\"\"\n",
    "    Computes GED-like MSE loss between predicted and true adjacency matrices.\n",
    "    \"\"\"\n",
    "    edge_logits_raw = output['edge_logits_raw']  # Expect shape [N, N] or [N*N]\n",
    "\n",
    "    # Determine number of nodes in prediction\n",
    "    if edge_logits_raw.dim() == 1:\n",
    "        # Flattened, so reshape\n",
    "        num_nodes = int(edge_logits_raw.size(0) ** 0.5)\n",
    "        adj_pred = edge_logits_raw.view(num_nodes, num_nodes)\n",
    "    elif edge_logits_raw.dim() == 2:\n",
    "        # Already square\n",
    "        adj_pred = edge_logits_raw\n",
    "        num_nodes = adj_pred.size(0)\n",
    "    else:\n",
    "        raise ValueError(\"edge_logits_raw must be 1D or 2D\")\n",
    "\n",
    "    # Safety check\n",
    "    if adj_pred.size(0) != adj_pred.size(1):\n",
    "        raise ValueError(\"Predicted adjacency matrix must be square\")\n",
    "\n",
    "    # Pad predicted adjacency\n",
    "    adj_pred_padded = pad_to_full_graph(adj_pred, full_size=full_size)\n",
    "\n",
    "    # True adjacency matrix\n",
    "    adj_true = torch.zeros(full_size, full_size, device=target.x.device)\n",
    "    adj_true[target.edge_index[0], target.edge_index[1]] = 1.0\n",
    "\n",
    "    # Match the shapes\n",
    "    if adj_pred_padded.shape != adj_true.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predicted {adj_pred_padded.shape}, true {adj_true.shape}\")\n",
    "\n",
    "    # MSE Loss\n",
    "    adj_mse_loss = F.mse_loss(adj_pred_padded, adj_true)\n",
    "    return adj_mse_loss\n",
    "\n",
    "\n",
    "# Combined loss (for training)\n",
    "def total_loss_fn(output, data):\n",
    "    node_loss = node_reconstruction_loss(output['node_logits'], data.x, data.masked_nodes)\n",
    "    edge_loss = edge_reconstruction_loss(output['edge_logits'], data.edge_index, data.x.size(0))\n",
    "\n",
    "    # Optional GED\n",
    "    ged_loss = graph_edit_distance_loss(output['edge_logits'], torch.zeros_like(output['edge_logits']))  # Replace with true adj if available\n",
    "\n",
    "    total = node_loss + edge_loss + 0.1 * ged_loss\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9576e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "def evaluate_edge_metrics(pred_adj, target_edge_index, threshold=0.5):\n",
    "    num_nodes = pred_adj.size(0)\n",
    "    true_adj = torch.zeros_like(pred_adj)\n",
    "    true_adj[target_edge_index[0], target_edge_index[1]] = 1.0\n",
    "\n",
    "    pred_binary = (pred_adj > threshold).float()\n",
    "\n",
    "    y_true = true_adj.view(-1).cpu().numpy()\n",
    "    y_pred = pred_binary.view(-1).cpu().numpy()\n",
    "\n",
    "    acc = (y_true == y_pred).sum() / y_true.shape[0]\n",
    "    f1 = f1_score(y_true, y_pred)  # binary\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    return acc, f1, int(tp), int(fp), int(fn), int(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "572e52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbatch_output(output, data_list):\n",
    "    \"\"\"\n",
    "    Splits batched output['edge_logits_raw'] into per-graph tensors.\n",
    "    Assumes edge_logits_raw is 1D concatenated logits from all graphs.\n",
    "    \"\"\"\n",
    "    edge_logits_raw = output['edge_logits_raw']\n",
    "    output_list = []\n",
    "\n",
    "    start = 0\n",
    "    for data in data_list:\n",
    "        n = data.num_nodes\n",
    "        size = n * n\n",
    "        raw = edge_logits_raw[start:start + size].view(n, n)\n",
    "        out_dict = {\n",
    "            'edge_logits_raw': raw,\n",
    "            'edge_logits': torch.sigmoid(raw),\n",
    "            'node_logits': output['node_logits'][data.batch == 0]  # If needed\n",
    "        }\n",
    "        output_list.append(out_dict)\n",
    "        start += size\n",
    "\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2358ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_edge_f1 = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        data = batch['input'].to(device)     # Batched input graphs\n",
    "        target = batch['target'].to(device)  # Batched target graphs\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # Node reconstruction loss (can be batched)\n",
    "        node_loss = node_reconstruction_loss(\n",
    "            output['node_logits'],\n",
    "            target.x,\n",
    "            data.masked_nodes\n",
    "        )\n",
    "\n",
    "        # Edge reconstruction loss (can be batched)\n",
    "        edge_loss = edge_reconstruction_loss(\n",
    "            output['edge_logits_raw'],\n",
    "            target.edge_index,\n",
    "            target.num_nodes\n",
    "        )\n",
    "\n",
    "        # GED loss (must be per graph in batch)\n",
    "        data_list = data.to_data_list()\n",
    "        target_list = target.to_data_list()\n",
    "        output_list = unbatch_output(output, data_list)  # Helper splits batched outputs\n",
    "\n",
    "        ged_loss = 0.0\n",
    "        for out, tgt in zip(output_list, target_list):\n",
    "            try:\n",
    "                ged_loss += graph_edit_distance_loss(out, tgt, full_size=45)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] GED Loss skipped for one graph: {e}\")\n",
    "                ged_loss += 0.0  # skip but don’t crash\n",
    "\n",
    "        ged_loss = ged_loss / len(output_list)\n",
    "\n",
    "        # Total loss\n",
    "        loss = node_loss + edge_loss + ged_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        # Edge metrics (optional: you can aggregate over whole batch too)\n",
    "        edge_acc, edge_f1, *_ = evaluate_edge_metrics(output['edge_logits'], target.edge_index)\n",
    "        total_edge_f1 += edge_f1\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    avg_edge_f1 = total_edge_f1 / batch_count\n",
    "\n",
    "    print(f\"Train Loss = {avg_loss:.4f} | Edge F1 avg batch = {avg_edge_f1:.2f}%\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6cc1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        data = batch['input'].to(device)\n",
    "        target = batch['target'].to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        node_loss = node_reconstruction_loss(\n",
    "            output['node_logits'],\n",
    "            target.x,\n",
    "            data.masked_nodes\n",
    "        )\n",
    "\n",
    "        edge_loss = edge_reconstruction_loss(\n",
    "            output['edge_logits'],\n",
    "            target.edge_index,\n",
    "            target.num_nodes\n",
    "        )\n",
    "\n",
    "        loss = node_loss + edge_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6896bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:25<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 1.3147 | Edge F1 avg batch = 0.06%\n",
      "Epoch 1: Train Loss = 1.3147, Val Loss = 1.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:30<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7849 | Edge F1 avg batch = 0.07%\n",
      "Epoch 2: Train Loss = 0.7849, Val Loss = 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:30<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7423 | Edge F1 avg batch = 0.09%\n",
      "Epoch 3: Train Loss = 0.7423, Val Loss = 0.9622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:31<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7268 | Edge F1 avg batch = 0.15%\n",
      "Epoch 4: Train Loss = 0.7268, Val Loss = 0.9588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:30<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7204 | Edge F1 avg batch = 0.23%\n",
      "Epoch 5: Train Loss = 0.7204, Val Loss = 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:31<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7192 | Edge F1 avg batch = 0.33%\n",
      "Epoch 6: Train Loss = 0.7192, Val Loss = 0.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:32<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7171 | Edge F1 avg batch = 0.36%\n",
      "Epoch 7: Train Loss = 0.7171, Val Loss = 0.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:30<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7164 | Edge F1 avg batch = 0.36%\n",
      "Epoch 8: Train Loss = 0.7164, Val Loss = 0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3240/3240 [05:33<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.7176 | Edge F1 avg batch = 0.39%\n",
      "Epoch 9: Train Loss = 0.7176, Val Loss = 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 617/3240 [01:02<04:23,  9.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, device)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[73], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Batched target graphs\u001b[39;00m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Node reconstruction loss (can be batched)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m node_loss \u001b[38;5;241m=\u001b[39m node_reconstruction_loss(\n\u001b[0;32m     20\u001b[0m     output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_logits\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m     target\u001b[38;5;241m.\u001b[39mx,\n\u001b[0;32m     22\u001b[0m     data\u001b[38;5;241m.\u001b[39mmasked_nodes\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[63], line 25\u001b[0m, in \u001b[0;36mGraphTransformer.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(x)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers:\n\u001b[1;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[0;32m     27\u001b[0m node_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\nn\\conv\\transformer_conv.py:229\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[0;32m    225\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor,\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m#                  edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m                     \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.transformer_conv_TransformerConv_propagate_q6y57dud.py:286\u001b[0m, in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, query, key, value, edge_attr, size)\u001b[0m\n\u001b[0;32m    274\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[0;32m    275\u001b[0m                 query_i\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mquery_i,\n\u001b[0;32m    276\u001b[0m                 key_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkey_j,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    283\u001b[0m             )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:594\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[1;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    579\u001b[0m     inputs: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[0;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\nn\\aggr\\base.py:131\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[1;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\nn\\aggr\\basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[1;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\nn\\aggr\\base.py:185\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[1;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch_geometric\\utils\\_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = GraphTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ed4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
